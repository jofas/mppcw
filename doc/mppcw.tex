\documentclass[twoside,11pt]{article}
\PassOptionsToPackage{hyphens}{url}
\usepackage{jmlr2e}
\usepackage{amsmath}
\usepackage[toc,page]{appendix}
\usepackage[table]{xcolor}
\usepackage[marginparsep=30pt]{geometry}
\usepackage{stmaryrd}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{tabu}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{listings}
\usepackage{fancyref}
\usepackage{relsize}
\usepackage{float}
\usepackage{subcaption}

\usetikzlibrary{%
    arrows,
    arrows.meta,
    decorations,
    backgrounds,
    positioning,
    fit,
    petri,
    shadows,
    datavisualization.formats.functions,
    calc,
    shapes,
    shapes.multipart,
    matrix,
    plotmarks
}

\usepgfplotslibrary{fillbetween, statistics}

\pgfplotsset{
  compat=1.3,
  every non boxed x axis/.style={
  enlarge x limits=false,
  x axis line style={}%-stealth},
  },
  every boxed x axis/.style={},
  every non boxed y axis/.style={
  enlarge y limits=false,
  y axis line style={}%-stealth},
  },
  every boxed y axis/.style={},
}

\def\perc{\texttt{perco\-late}}
\def\v{\texttt{v0.1.0}}

\def\titl{Message Passing Programming coursework:
  optimization of \perc{} \v{} using a 2d domain
  decomposition with MPI}

\title{\titl}

\author{}

\ShortHeadings{B160509}{B160509}
\firstpageno{1}


\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\begin{keywords}
Scientific programming, benchmark, parallelization,
performance optimization, MPI
\end{keywords}

\section{Introduction} % {{{

\perc{} \v{} is a scientific program written in the Fortran
programming language. It generates a random matrix with two
kinds of cells: empty and filled. Empty cells build
clusters with their neighboring empty cells.
\perc{} computes all clusters in the matrix and searches
for a cluster that makes the matrix percolate.
The matrix percolates, if there exits a cluster that begins
in the left most column of the matrix and ends in the right
most column.

The computing of the clusters---the clustering---is done
iteratively and is the main cause of computation in
\perc{}.
If the clustering is done in a serial way, it does not
scale well and clustering bigger matrices can consume a
lot of time and power.

This paper presents a parallelized version of \perc{}.
The parallel version decomposes the matrix into smaller
chunks and distributes them among worker instances.
Every worker performs the clustering of its chunk and
communicates with the neighboring worker
instances through halo swapping, which ensures that the
whole matrix is clustered.
Chunks are generated by splitting the matrix on both axes.
This makes it a 2d domain decomposition of the matrix.
The parallel version is based on MPI and the worker
instances are MPI processes \citep[see][]{mpi}.

First, this paper describes the clustering algorithm used
by \perc{} and the way \perc{} is parallelized using the
MPI library.
The parallel version's correctness is tested by a
regression test suite, which is briefly outlined.
Afterwards a benchmark is presented, which analyzes the
scaling behavior of the parallel version over multiple
amounts of MPI processes and with different sized matrices.
The results of the benchmark are discussed and a conclusion
is drawn.

% }}}

\section{Method} % {{{

This chapter presents a mathematical definition of the
clustering algorithm used by \perc{}.
The poor running time of the serial version of the
clustering algorithm is shown.
Afterwards the parallelized version of the clustering is
described and a brief outline of the regression test suite
for testing the correctness of the parallel version is
given.
At last, this chapter presents the benchmark, that is
discussed in the following chapters.

\subsection{Mathematical definition of the clustering
  algorithm used by \perc{}}

Let $A \in \mathbb{N}_0^{n \times n}$ be the matrix that
is clustered by \perc{}.
Let $A(i, j);1\leq i, j \leq n$ be the element at the
$i$th row and $j$th column of $A$.
An element from $A$ has a state. It is either empty or
filled.
\perc{} randomly initializes $A$ with empty and filled
elements.
The density of the filled elements $\rho_{goal}$ can be
defined as a parameter, which is approximated during
initialization. $n$, as well, is provided as a parameter to
\perc{}.

Let $\sigma : \mathbb{N}_0 \rightarrow \{empty,filled\}$ be
a function mapping a non-negative integer to its state:
\begin{align*}
  \sigma(x) := \begin{cases}
    filled &\text{if } x = 0 \\
    empty  &\text{otherwise}
  \end{cases}.
\end{align*}

\begin{proposition}
  \label{prop:one}
  Let $x, y \in \mathbb{N}_0$.
  For every $x: \sigma(x) = empty$, follows: $x > y$, if
  $\sigma(y) = filled$.
\end{proposition}

\begin{proof}
  There exits no smaller non-negative integer than $0$.
  $\sigma(y) = filled$, only for $y = 0$. Therefore, every
  number $x$, for which $\sigma(x) = empty$, must be bigger
  than $y$.
\end{proof}

Let $\mu$ be the function that returns the maximum value
of an element and its neighbors:
\begin{align*}
  \mu(A, i, j) := \max(A(i,j), A(i-1,j), A(i+1,j),
                       A(i,j-1), A(i,j+1)).
\end{align*}
For now, if $i$ or $j$ equals $0$ or $n + 1$ (in other
words, if $i$ or $j$ violate the boundaries of $A$), then
$A(i, j)$ will return $0$.

\begin{proposition}
  \label{prop:two}
  If $\sigma(A(i, j)) = empty$, than
  $\sigma(\mu(A, i, j)) = empty$ as well.
\end{proposition}

\begin{proof}
  $\mu(A, i, j)$ returns the maximum of the element
  $A(i, j)$ and its neighbors, wherefore $\mu(A, i, j) \geq
  A(i, j)$.
  From Proposition~\ref{prop:one} follows, that every
  number $y \in \mathbb{N}_0: \sigma(y) = filled$ must be
  less than $A(i, j)$, if $\sigma(A(i,j)) = empty$.
\end{proof}

With Proposition~\ref{prop:two}, we can now safely derive
a recursive definition of the clustering operation (because
$\mu$ will never change the state of an empty cell to
filled).
First, let $c_{step}:\mathbb{N}_0^{n \times n} \rightarrow
\mathbb{N}_0^{n \times n}$ be a single clustering step,
that maps every empty element of $A$ to its biggest
neighbor and leaves filled elements untouched:
\begin{align*}
  c_{step}(A) := i,j=1,\dots,n: \begin{cases}
    \mu(A, i, j) &\text{if } \sigma(A(i, j)) = empty \\
    0 &\text{otherwise}.
  \end{cases}
\end{align*}
The clustering operation $c$ of \perc{} can now be defined
as a recursive function, that executes $c_{step}$, as long
as it continues to change empty elements to their biggest
neighbor:
\begin{align*}
  c(A) := \begin{cases}
    A &\text{if } c_{step}(A) = A \\
    c(c_{step}(A)) &\text{otherwise}.
  \end{cases}
\end{align*}

Imagine the case where for every element
$\sigma(A(i,j)) = filled$ follows, that
$\mu(A, i, j) = A(i, j)$.
This would make $c(A)$ call $c_{step}$ just a single time,
resulting in the best case running time of $c$:
$\Omega(c) = n^2$, which---for bigger $n$---is still quite
slow, even though it is the best case.

After the clustering, it is easy to check whether the
matrix percolates.
The matrix percolates, if $\exists i, \exists j: A(i,1) =
A(j,n)$.
In other words, if any element in the first column has the
same value as any element in the last column of $A$, the
matrix percolates.


\subsection{Parallelized version of \perc{} using MPI}

To tackle the poor performance, \perc{} was parallelized
using the MPI Standard, version 3.1 \citep[see][]{mpi}.

Let $p$ be the amount of MPI processes the parallel version
of \perc{} is executed with.
The processes are arranged in a virtual, two dimensional
cartesian topology \citep[see][Chapter 7]{mpi}.

% graph with example of splitting map

% thoroughly describe percolate_par

\subsection{Regression test suite for testing the
  correctness of the parallel version of \perc{}}

% briefly outline regression test suite

\subsection{The benchmark discussed in the following
  chapters}

% describe benchmark

% }}}

\section{Results} % {{{

% discuss benchmark

% plot: speedup (with optimal speedup) (big, avg over all
%       matrix sizes and seeds), marker for points

% table: processes -> matrix size (avg. over the seeds)

% }}}

\section{Discussion} % {{{

% }}}

\section{Conclusion} % {{{

% }}}

\bibliography{mppcw.bib}

\end{document}
