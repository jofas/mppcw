\documentclass[twoside,11pt]{article}
\PassOptionsToPackage{hyphens}{url}
\usepackage{jmlr2e}
\usepackage{amsmath}
\usepackage[toc,page]{appendix}
\usepackage[table]{xcolor}
\usepackage[marginparsep=30pt]{geometry}
\usepackage{stmaryrd}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{tabu}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{listings}
\usepackage{fancyref}
\usepackage{relsize}
\usepackage{float}
\usepackage{subcaption}
\usepackage{diagbox}

\usetikzlibrary{%
    arrows,
    arrows.meta,
    decorations,
    backgrounds,
    positioning,
    fit,
    petri,
    shadows,
    datavisualization.formats.functions,
    calc,
    shapes,
    shapes.multipart,
    matrix,
    plotmarks
}

\usepgfplotslibrary{fillbetween, statistics}

\pgfplotsset{
  compat=1.3,
  every non boxed x axis/.style={
  enlarge x limits=false,
  x axis line style={}%-stealth},
  },
  every boxed x axis/.style={},
  every non boxed y axis/.style={
  enlarge y limits=false,
  y axis line style={}%-stealth},
  },
  every boxed y axis/.style={},
}

\def\perc{\texttt{perco\-late}}
\def\v{\texttt{v0.1.0}}

\def\titl{Message Passing Programming coursework:
  optimization of \perc{} \v{} using a 2d domain
  decomposition with MPI}

\title{\titl}

\author{}

\ShortHeadings{B160509}{B160509}
\firstpageno{1}


\begin{document}

\maketitle

\begin{abstract}
This paper describes an optimized version of \perc{} \v{}.
\perc{} is a scientific program, which generates a random
matrix with two kinds of elements: empty and filled.
Empty elements build clusters with their neighboring empty
elements and \perc{} finds clusters that begin at the first
column and end at the last.
If such a cluster exists, the matrix percolates.

\perc{} performs a very costly clustering operation,
wherefore the need for a faster solution arises.

This paper describes the clustering operation and
demonstrates its poor performance, by analyzing its best
case time complexity, before going into detail about how
MPI was used for parallelizing \perc{} and how its
correctness is tested with a regression test suite.
Also, a benchmark is presented, which successfully shows
the performance benefits of the parallel version and its
scalability.
The results of the benchmark are presented and discussed
in this paper, before at last a conclusion is drawn.

\end{abstract}

\begin{keywords}
Scientific programming, benchmark, parallelization,
performance optimization, MPI
\end{keywords}

\section{Introduction} % {{{

\perc{} \v{} is a scientific program written in the Fortran
programming language. It generates a random matrix with two
kinds of elements: empty and filled. Empty elements build
clusters with their neighboring empty elements.
\perc{} computes all clusters in the matrix and searches
for a cluster that makes the matrix percolate.
The matrix percolates, if there exits a cluster that begins
in the left most column of the matrix and ends in the right
most one.

The clustering is done iteratively and is the main cause of
computation in \perc{}.
If it is done in a serial way, it does not
scale well and clustering bigger matrices can consume a
lot of time and power.

This paper presents a parallelized version of \perc{}.
The parallel version decomposes the matrix into smaller
chunks and distributes them among worker instances.
Every worker performs the clustering of its chunk and
communicates with the neighboring worker
instances through halo swapping, which ensures that the
whole matrix is clustered.
Chunks are generated by splitting the matrix on both axes.
This makes it a 2d domain decomposition of the matrix.
The parallel version is based on MPI and the worker
instances are MPI processes \citep[see][]{mpi}.

This paper begins by describing the clustering algorithm
used by \perc{} and the way \perc{} is parallelized using
the MPI library.
The parallel version's correctness is tested by a
regression test suite, which is briefly outlined.
Afterwards a benchmark is presented, which analyzes the
scaling behavior of the parallel version over multiple
amounts of MPI processes and with different sized matrices.
The results of the benchmark are discussed and a conclusion
is drawn.

% }}}

\section{Method} % {{{

This chapter presents a mathematical definition of the
clustering algorithm used by \perc{}.
The poor time complexity of the serial version of the
clustering algorithm is shown.
Afterwards the parallelized version of the clustering is
described and a brief outline of the regression test suite
for testing the correctness of the parallel version is
given.
At last, this chapter presents the benchmark, that is
discussed in the following chapters.

% math def {{{

\subsection{The clustering algorithm}

Let $A \in \mathbb{N}_0^{n \times n}$ be the matrix that
is clustered by \perc{}.
Let $A(i, j);1\leq i, j \leq n$ be the element at the
$i$th row and $j$th column of $A$.
An element from $A$ has an immutable state. It is either
empty or filled.
\perc{} randomly initializes $A$ with empty and filled
elements.
The density of the filled elements $\rho_{goal}$ can be
defined as a parameter, which is approximated during
initialization. $n$, as well, is provided as a parameter to
\perc{}.

Let $state : \mathbb{N}_0 \rightarrow \{empty,filled\}$ be
a function mapping a non-negative integer to its state:
\begin{align*}
  state(x) := \begin{cases}
    filled &\text{if } x = 0 \\
    empty  &\text{otherwise}
  \end{cases}.
\end{align*}

\begin{proposition}
  \label{prop:one}
  Let $x, y \in \mathbb{N}_0$.
  For every $x: state(x) = empty$, follows: $x > y$, if
  $state(y) = filled$.
\end{proposition}

\begin{proof}
  There exits no smaller non-negative integer than $0$.
  $state(y) = filled$, only for $y = 0$. Therefore, every
  number $x$, for which $state(x) = empty$, must be bigger
  than $y$.
\end{proof}

Let $\mu$ be the function that returns the maximum value
of an element and its neighbors:
\begin{align*}
  \mu(A, i, j) := \max(A(i,j), A(i-1,j), A(i+1,j),
                       A(i,j-1), A(i,j+1)).
\end{align*}
For now, if any index equals $0$ or $n + 1$ (if it violates
the boundaries of $A$), then $A(i, j)$ will return $0$.

\begin{proposition}
  \label{prop:two}
  If $state(A(i, j)) = empty$, than
  $state(\mu(A, i, j)) = empty$ as well.
\end{proposition}

\begin{proof}
  $\mu(A, i, j)$ returns the maximum of the element
  $A(i, j)$ and its neighbors, wherefore $\mu(A, i, j) \geq
  A(i, j)$.
  From Proposition~\ref{prop:one} follows, that every
  number $y \in \mathbb{N}_0: state(y) = filled$ must be
  less than $A(i, j)$, if $state(A(i,j)) = empty$.
\end{proof}

With Proposition~\ref{prop:two}, we can now safely derive
a recursive definition of the clustering operation (because
$\mu$ will never change the state of an empty cell to
filled, aligning it with the immutability property of an
element's state).
First, let $c_{step}:\mathbb{N}_0^{n \times n} \rightarrow
\mathbb{N}_0^{n \times n}$ be a single clustering step,
that maps every empty element of $A$ to its biggest
neighbor and leaves filled elements untouched:
\begin{align*}
  c_{step}(A) := i,j=1,\dots,n: \begin{cases}
    \mu(A, i, j) &\text{if } state(A(i, j)) = empty \\
    0 &\text{otherwise}
  \end{cases}.
\end{align*}
The clustering operation $c$ of \perc{} can now be defined
as a recursive function, that executes $c_{step}$, as long
as it continues to change empty elements to their biggest
neighbor:
\begin{align*}
  c(A) := \begin{cases}
    A &\text{if } c_{step}(A) = A \\
    c(c_{step}(A)) &\text{otherwise}
  \end{cases}.
\end{align*}

Imagine the case where for every element
$state(A(i,j)) = filled$ follows, that
$\mu(A, i, j) = A(i, j)$.
This would make $c(A)$ call $c_{step}$ just a single time,
resulting in the best case running time of $c$:
$\Omega(c) = n^2$.
Any time complexity of $n^2$ is quite slow, especially
if it is the best case time complexity.
This makes $c$ unfeasible for bigger $n$, especially if
$\rho_{goal}$ is set to a high value (close to 1), because
this will increase the depth of the recursion.

After the clustering, it is easy to check whether the
matrix percolates.
The matrix percolates, if $\exists i, \exists j: A(i,1) =
A(j,n)$.
In other words, if any element in the first column has the
same value as any element in the last column of $A$, the
matrix percolates.

% }}}

% parallel {{{

\subsection{Parallelized version of \perc{} using MPI}

To tackle the poor performance of the clustering algorithm,
\perc{} was parallelized using the MPI Standard,
version 3.1 \citep[see][]{mpi}.
Algorithm~\ref{alg:perc_par} shows the parallel version of
\perc{}.
This chapter outlines the implementation of the parallel
version and its intricacies.

Let $p$ be the amount of MPI processes the parallel version
of \perc{} is executed with.
Every process has a rank, which is drawn from a sequence
$0,1,\dots,p-1$.
The process with rank 0 is called the root process.

The processes are arranged in a virtual, two dimensional
Cartesian topology.
The $p$ processes are as evenly distributed as possible
over both axes of the topology with the
\texttt{MPI\_Dims\_create} routine.
\texttt{MPI\_Dims\_create} returns an array $dims$ with two
elements, one for each axis of the topology.
$dims_{row} = dims(1)$ contains the amount of processes the
rows of $A$ are split by, $dims_{column} = dims(2)$ the
amount of processes the columns of $A$ are split by.
After generating the dimensions of the topology,
\texttt{MPI\_Cart\_create} is used to generate the actual
communicator that manages the virtual topology from
\texttt{MPI\_COMM\_WORLD} \citep[see][Chapter 7]{mpi}.

The previous chapter states, that $A(i, j) = 0$, if $i$ or
$j$ equals $0$ or $n + 1$ ($i$ or $j$ out of bounds).
The actual clustering of \perc{} behaves differently.
$A(i, j) = 0$, only if $j$ is out of bounds.
If $i$ is out of bounds, then a periodic boundary condition
is used: $A(0, j) = A(n, j), A(n + 1, j) = A(1, j)$.
This condition is easily implemented with
\texttt{MPI\_Cart\_create}, which actually has an argument
\texttt{periods}. \texttt{periods} is an array with two
elements (for each axis) containing boolean values, whether
an axis of the topology is periodic or not
\citep[see][Chapter 7]{mpi}.

The root process initializes $A$, which needs to be
distributed to the MPI processes (see
Algorithm~\ref{alg:perc_par}, lines 2ff).
Every process has coordinates in the topology, based on
its rank.
The coordinates of a process come from the
\texttt{MPI\_Cart\_coords} routine
\citep[see][Chapter 7]{mpi}.

Let $coords_{p_i}$ be the coordinates of the process with
rank $p_i$ and let $coords_{p_i, row}$ be the coordinates'
value for the first axis and $coords_{p_i, column}$ be its
value for the second axis of the topology.
$coords$ behaves like the ranks, ranging from $0$ to
$dims_{row} - 1$ and $0$ to $dims_{column} - 1$,
respectively \citep[see][Chapter 7]{mpi}.

The chunk of $A$, which is assigned to the process with
rank $p_i$ can now be defined by a tuple
$\alpha_{p_i} := (i, j, l, m)$.
$i$ and $j$ are the indices of $A$, which point to the
first element of the chunk, while $l$ is the amount of rows
and $m$ the amount of columns the chunk possesses.
Let $s_{row} := \lfloor \frac{n}{dims_{row}} \rfloor$ and
$s_{column} := \lfloor \frac{n}{dims_{column}} \rfloor$ be
the general size of the splits of $A$ for both axes.
With use of these split values and $coords$ and $dims$, we
can define $i, j, l$ and $m$:
\begin{align*}
  &i := coords_{p_i,row} \cdot s_{row} + 1 \\
  &j := coords_{p_i, column} \cdot s_{column} + 1 \\
  &l := \begin{cases}
    s_{row} + n \mod dims_{row} &\text{if } coords_{p_i, row} = dims_{row} - 1 \\
    s_{row} &\text{otherwise}
  \end{cases} \\
  &m := \begin{cases}
    s_{column} + n \mod dims_{column} &\text{if } coords_{p_i, column} = dims_{column} - 1 \\
    s_{column} &\text{otherwise}
  \end{cases}.
\end{align*}

For both axes of $A$, every process has a chunk of the same
size, except the processes, which coordinates' value for
the equivalent axis in the topology is the highest possible
value for it ($dims_{row} - 1$ or $dims_{column} - 1$).
The last process has a chunk of the same size as the other
processes, plus the rest of the elements of $A$ among that
axis (see Figure~\ref{fig:example_distribution}).

$l$ and $m$ are needed in order to generate a strided
vector type with \texttt{MPI\_Type\_vector}, which is used
for sending the chunk from the root process to $p_i$ and
vice versa.
In this case, the type for sending $p_i$'s chunk would be
generated with setting \texttt{count} to $m$,
\texttt{blocklength} to $l$ and \texttt{stride} to $n$
\citep[see][Chapter 4]{mpi}.

The chunks are scattered from the root process to every
non-root process with \texttt{MPI\_S\-send}.
The root process simply copies its chunk from $A$.
Gathering, after the clustering is finished works the same
way as the scattering, just reverse (the non-root processes
sending their chunks back to the root process with
\texttt{MPI\_Ssend} and the root process copying its chunk
back to $A$) \citep[see Algorithm~\ref{alg:perc_par},
lines 5,7 and][Chapter 3]{mpi}.

\begin{algorithm} % {{{
  \caption{: parallel version of \perc{}}
  \label{alg:perc_par}

  \begin{algorithmic}[1]
    \STATE{initialize MPI and the Cartesian topology}
    \IF{rank = 0}
      \STATE{randomly initialize $A$}
    \ENDIF
    \STATE{scatter $A$ to every process's chunk}
    \STATE{execute $c_{par}$(chunk) (Algorithm~\ref{alg:cluster_par})}
    \STATE{gather the chunks back to $A$}
    \IF{rank = 0}
      \STATE{find out if $A$ percolates}
      \STATE{save $A$ as a Portable Gray Map file}
    \ENDIF
    \STATE{finalize MPI}

  \end{algorithmic}
\end{algorithm} % }}}

Every process has four neighbors: a left, right, upper and
lower neighbor (see Figure~\ref{fig:example_neighbors}).
The neighbors are needed for swapping halos, so the
clustering is actually done over the whole matrix, and not
just over the chunks.
The neighbors are determined with
\texttt{MPI\_Cart\_shift}, shifting both axes of the
topology up one element \citep[see][Chapter 7]{mpi}.

The second axis of the topology is not periodic, so
processes for which $coords_{p_i, column} = 0$ have
\texttt{MPI\_PROC\_NULL} as their left neighbor.
The same goes for processes for which
$coords_{p_i, column} = dims_{column} - 1$, only their
right neighbor is set to \texttt{MPI\_PROC\_NULL}
\citep[see Figure~\ref{fig:example_neighbors} and]
[Chapter 3]{mpi}.

$p_i$'s chunk is the actual chunk it is provided from $A$
by the root process, plus a halo (chunk$: L+2 \times M+2$,
its indices ranging from $0,\dots,L+1$ and $0,\dots,M+1$).
The halo is used as a container for the data received from
$p_i$'s neighbors during the halo swapping, or as a buffer
of empty elements for $\mu$, if the process's left or right
neighbor is \texttt{MPI\_PROC\_NULL}.

The halo swapping happens before each $cluster_{step}$
(see Algorithm~\ref{alg:cluster_par}, line 3).
Halo swapping means, the outer most row or column (in
all directions: left, right upper and lower) is send
to the corresponding neighbor, which sends its outer most
row or column in return (the opposite of the direction it
receives from).
In the example shown in Figure~\ref{fig:example}, $p_4$
would send its first row $(5,6)$ to $p_2$, its upper
neighbor and would receive $(0,3)$ from $p_2$ (its lower
and only row) in return, which is then stored in
chunk$_{p_4}(0, 1:2)$.

The halo swapping is realized with \texttt{MPI\_Sendrecv}.
Because the first axis of the topology is periodic,
\texttt{MPI\_Sendrecv} can not be called with the same
neighbor (e.g. sending to upper and receiving from upper).
Instead, \texttt{MPI\_Sendrecv} must be called with
upper and lower for sending and receiving (sending to
upper and receiving from lower and vice versa).
Otherwise \perc{} would deadlock
\citep[see][Chapter 3]{mpi}.

For stopping the clustering, every process computes the
sum over each element in its chunk (without the halos).
The sum is then reduced and broadcast to every process
with \texttt{MPI\_Allreduce} \citep[see][Chapter 5]{mpi}.
If the reduced sum is equal to the sum generated by the
previous step, the clustering is finished (see
Algorithm~\ref{alg:cluster_par}).

\begin{algorithm} % {{{
  \caption{: $c_{par}$(chunk)}
  \label{alg:cluster_par}

  \begin{algorithmic}[1]
    \STATE{$sum_{A'} := 0$}
    \WHILE{$true$}
      \STATE{swap halos with neighbors}
      \STATE{chunk $:= c_{step}($chunk$)$}
      \STATE{$sum := \sum_{i=1}^{l}\sum_{j=1}^{m}$chunk$(i,j)$}
      \STATE{reduce $sum$ over all processes into $sum_{A}$}
      \IF{$sum_{A} = sum_{A'}$}
        \STATE{exit loop}
      \ENDIF
      \STATE{$sum_{A'} := sum_{A}$}
    \ENDWHILE
  \end{algorithmic}
\end{algorithm} % }}}

\begin{figure} % {{{
\begin{subfigure}[t]{\textwidth}
\begin{center}
\begin{tikzpicture} % {{{
  \matrix[anchor=east, label=$A$]
  at (0, 0) (A)
  {
    \node (ps)   {0}; & \node (pvs) {1}; & \node        {0};
                      & \node (pp1) {2}; & \node        {0}; \\
    \node (ph1s) {0}; & \node       {3}; & \node        {0};
                      & \node       {0}; & \node (ph1e) {4}; \\
    \node (ph2s) {5}; & \node       {6}; & \node        {7};
                      & \node       {0}; & \node (ph2e) {8}; \\
    \node        {0}; & \node       {0}; & \node        {9};
                      & \node      {10}; & \node        {0}; \\
    \node (pp4)  {0}; & \node (pve) {0}; & \node       {11};
                      & \node (pp5){12}; & \node (pe)  {13}; \\
  };

  \draw (ps.north west) -| (pe.south east) -| cycle;
  \draw (ph1s.north west) -- (ph1e.north east);
  \draw (ph2s.north west) -- (ph2e.north east);
  \draw (pvs.north east) -- (pve.south east);

  \matrix[above left=1.42cm of A.north west, label=$p_0$, draw] (p0)
  {
    \node[anchor=west]{$coords_{p_0} = (0, 0)$};\\
    \node[anchor=west]{$\alpha_{p_0} = (1, 1, 1, 2)$};\\
  };

  \matrix[above right=1.42cm of A.north east, label=$p_1$, draw] (p1)
  {
    \node{$coords_{p_1} = (0, 1)$};\\
    \node{$\alpha_{p_1} = (1, 3, 1, 3)$};\\
  };
  \matrix[left=1cm of A.west, label=$p_2$, draw] (p2)
  {
    \node{$coords_{p_2} = (1, 0)$};\\
    \node{$\alpha_{p_2} = (2, 1, 1, 2)$};\\
  };
  \matrix[right=1cm of A.east, label=$p_3$, draw] (p3)
  {
    \node{$coords_{p_3} = (1, 1)$};\\
    \node{$\alpha_{p_3} = (2, 3, 1, 3)$};\\
  };
  \matrix[below left=1.42cm of A.south west, label=$p_4$, draw] (p4)
  {
    \node{$coords_{p_4} = (2, 0)$};\\
    \node{$\alpha_{p_4} = (3, 1, 3, 2)$};\\
  };
  \matrix[below right=1.42cm of A.south east, label=$p_5$, draw] (p5)
  {
    \node{$coords_{p_5} = (2, 1)$};\\
    \node{$\alpha_{p_5} = (3, 3, 3, 3)$};\\
  };

  \draw (p0) -| (ps.north east);
  \draw (p1) -| (pp1);
  \draw (p2) -| ($(p2.east) !.5! (ph1s.west)$) |- (ph1s);
  \draw (p3) -| ($(p3.west) !.5! (ph1e.east)$) |- (ph1e);
  \draw (p4) -| (pp4.south east);
  \draw (p5) -| (pp5);
\end{tikzpicture} % }}}
\caption{Graph displaying how a randomly initialized
  $5 \times 5$ matrix is distributed among 6 processes.}
\label{fig:example_distribution}
\end{center}
\vspace{1cm}
\end{subfigure}
\begin{subfigure}[t]{\textwidth}
\begin{center}
\begin{tikzpicture} % {{{
  \matrix[column sep=1cm, row sep=1cm]{
    &\node (p0) {$p_0$}; &\node (p1) {$p_1$}; &\\
    \node (nullw)  {\texttt{MPI\_PROC\_NULL}};
    &\node (p2) {$p_2$}; &\node (p3) {$p_3$};
      &\node (nulle) {\texttt{MPI\_PROC\_NULL}}; \\
    &\node (p4) {$p_4$}; &\node (p5) {$p_5$}; \\
  };

  \draw[->] (p0) -| (nullw);
  \draw[->] (p2) -- (nullw);
  \draw[->] (p4) -| (nullw);

  \draw[-] (p0) -- (p2);
  \draw[-] (p2) -- (p4);

  \draw[-] (p0) -- (p1);
  \draw[-] (p4) -- (p5);

  \draw[->] (p1) -| (nulle);
  \draw[->] (p3) -- (nulle);
  \draw[->] (p5) -| (nulle);

  \draw[-] (p1) -- (p3);
  \draw[-] (p3) -- (p5);

  \draw[-] (p2) -- (p3);

  \draw[-] (p0) |- ($(p0.north west) + (-4cm,.5cm)$) --
             ($(p4.south west) - (4cm,.5cm)$) -| (p4);

  \draw[-] (p1) |- ($(p1.north east) + (4cm,.5cm)$) --
             ($(p5.south east) + (4cm,-.5cm)$) -| (p5);

\end{tikzpicture} % }}}
\caption{Directed graph showing the neighbors of each
  process.}
\label{fig:example_neighbors}
\end{center}
\vspace{1cm}
\end{subfigure}
\caption{Example of how $A: 5 \times 5$ is distributed
  among 6 processes. Also shows the neighbors of each
  process with which the halo swapping is done.}
\label{fig:example}
\end{figure} % }}}

In order to insure correctness of the result, the first
axis of the topology can not contain more elements than $n$
($dims_{row} \leq n$), because this axis uses a periodic
boundary condition.
If $dim_{row} > n$, than $s_{row} = 0$. That means, only
the processes with $coords_{p_i, row} = dims_{row} - 1$
would contain elements and all other processes would
contain empty chunks, because for them $l = s_{row} = 0$,
rendering the parallel version worse than the serial
version (if only this axis is considered), because a single process does the
clustering (like the serial version) and a communicative
overhead is introduced.
If not for the periodic boundary condition, this would only
endanger the performance of \perc{}, not its correctness.
But during halo swapping, a process with $coords_{p_i, row}
= 0$ would receive its lower halo from a process with
$coords_{p_i, row} = dims_{row} - 1$, which means this
particular halo swap is lost to the processes not
containing empty chunks (which should actually have
themselves as lower and upper neighbor), violating the
periodic boundary condition of the first axis.

Therefore, the Cartesian communicator truncates
$dim_{row} = n$, if $dim_{row} > n$, which means there are
$n$ processes among the first axis of the topology, each
containing a chunk which is the subset of a single row of
$A$.
Because empty processes only produce an overhead of
communication and therefore endanger performance, the same
truncation happens for $dim_{column}$.

% }}}

% test suite {{{

\subsection{The regression test suite}

The correctness of the parallel version of \perc{} is
tested with an expandable regression test suite, included
in the project.
All tests ran at this point were successful.
The test suite tests the output of the parallel version of
\perc{} against output of the serial version with the same
parameters.
The output, in this cases, is the generated Portable Gray
Map file (see Algorithm~\ref{alg:perc_par}, line 10).
If the files are identical, the test is successful.

The parallel version of \perc{} was tested against the
serial version with the following parameters:
$n := 2^0, 2^1,\dots,2^9$.
Each $n$ was combined with a seed for the random number
generator: $seed := 1560, 1561,\dots, 1564$.
The parallel version was executed with 1 to 4 processes on
a single Linux machine running Open MPI, version $4.0.2$
and 32 and 64 processes on two nodes of the Cirrus
supercomputer running HPE MPT version 2.16, with 32
processes per node \citep[see][]{openmpi, cirrus}.

% }}}

% benchmark {{{

\subsection{The benchmark}
\label{subsec:bench}

The benchmark presented below tests the performance and
scalability of the parallel version of \perc{}.
It was executed on eight back end nodes of the Cirrus
supercomputer with exclusive access \citep[see][]{cirrus}.

Measured were the clustering plus the scattering and
gathering of $A$.
Initialization and io were not part of the measurements.
\texttt{MPI\_Wtime} was used for measuring the execution
time \citep[see][Chapter 8]{mpi}.

The benchmark executed \perc{} with $2^0,2^1,\dots2^8$
processes, 32 processes per node.
Each was tested with $n := 2000, 3000, 4000, 5000$
and ten different seeds (one to ten), resulting in 360
distinct time measurements.

\perc{} was compiled with the Intel Fortran Compiler
(\texttt{ifort}) version 18.0.5, with the maximum serial
optimization provided by the compiler (optimization level
\texttt{O3}) \citep[see][]{ifort}.

% }}}

% }}}

\section{Results} % {{{

Figure~\ref{fig:speedup} shows the speedup of the parallel
version of \perc{} with more processes.
Every time measurement was grouped by the amounts of
processes $p$ the measurement was taken with.
The average over the grouped timings were used to compute
the speedup.
\perc{}'s parallel version's speedup is very close to the
optimal speedup.
The biggest difference between the optimal and the actual
speedup happens for 256 processes.
The actual speedup is still approximately 89 percent of the
optimum.

Table~\ref{tab:t} shows the average time in seconds and
the standard deviation over every benchmarked seed per
amount of processes $p$ and size of the matrix $n$.
For all $p$ and $n$, the average execution time and the
standard deviation decreases for bigger $p$ and smaller
$n$.

\begin{figure} % {{{
  \begin{center}
    \begin{tikzpicture}[scale=1.75] %{{{
  \datavisualization[
    scientific axes=clean,
    visualize as line/.list={d1, d},
    style sheet=vary dashing,
    style sheet=cross marks,
    d1={
      label in legend={text={optimal speedup}}
    },
    d={
      label in legend={text={actual speedup}},
    },
    y axis={
      logarithmic,
      ticks={major={at={1,2,4,8,16,32,64,128,256}} },
      label={speedup $\frac{\emptyset time_{p_1}}{\emptyset time_{p}}$ (\textit{log} scale)}
    },
    x axis={
      logarithmic,
      ticks={major={at={1,2,4,8,16,32,64,128,256}} },
      label={amount of processes $p$ (\textit{log} scale)}
    },
  ]
  data[headline={x, y}, read from file=data/speedup_data.csv, set=d]
  data[headline={x, y}, read from file=data/speedup_optimal.csv, set=d1]
  ;
    \end{tikzpicture} % }}}
  \end{center}
  \caption{Speedup when using more processes. The speedup
    was calculated over the average of every measurement,
    grouped by the amount of processes.}
  \label{fig:speedup}
\end{figure} % }}}

\begin{table} % {{{
  \begin{center}
  \begin{tabu} to \textwidth {l||rr|rr|rr|rr}
    &\multicolumn{2}{c|}{2000} &\multicolumn{2}{c|}{3000}
    &\multicolumn{2}{c|}{4000} &\multicolumn{2}{c}{5000} \\

    $p$
    &\multicolumn{1}{c}{time} &\multicolumn{1}{c|}{$\sigma$}
    &\multicolumn{1}{c}{time} &\multicolumn{1}{c|}{$\sigma$}
    &\multicolumn{1}{c}{time} &\multicolumn{1}{c|}{$\sigma$}
    &\multicolumn{1}{c}{time} &\multicolumn{1}{c}{$\sigma$} \\

    \hline
    \input{data/table}
  \end{tabu}
  \end{center}
  \caption{The average execution time and standard
    deviation $\sigma$ per amount of processes $p$ and
    matrix size $n$.
    Each combination of $p$ and $n$ was executed with ten
    different seeds (see Chapter~\ref{subsec:bench}).}
  \label{tab:t}
\end{table} % }}}

% }}}

\section{Discussion} % {{{

The benchmark successfully shows the performance benefits
and scalability of the parallel version of \perc{}.
The fact that the actual speedup is very close to the
optimal speedup is an indication for the implementation's
efficiency and the fact, that the overhead of communication
between processes is negligible in comparison to the
performance gains.
Overall can the benchmark be described as rather
unremarkable, in the sense that it does not reveal any
issues with the implementation, rather demonstrating its
strong scalability.

Also the fact that the average time and standard deviation
displayed in Table~\ref{tab:t} continuously decrease with
smaller $n$ and bigger $p$ show the robustness of the
parallel version of \perc{}.
The fact, that the speedup slightly drops with higher
amounts of processes could be the result of added distance
between them.
For example, the tests with 256 processes were distributed
among eight nodes of Cirrus, while the tests with 128
processes were only distributed among half of that.
This adds distance between processes in form of higher
network latency, which can decrease the performance of
the program.

A ceiling for the scalability of the parallel version,
where it hits a plateau and performance does not increase
anymore, or only with an exponential amount of more
processes, was not determined by the benchmark.

% }}}

\section{Conclusion} % {{{

The parallel version of \perc{} truly increases the
performance of the program, which is shown by the conducted
benchmark.
The fact that the actual speedup of the parallel version is
close to the optimal speedup (see Figure~\ref{fig:speedup}),
shows its strong scaling capabilities.
It makes the overhead of using message passing in form of
the MPI library negligible in comparison to the performance
gains.

Therefore, based on the results presented in this paper,
the parallel version of \perc{} is deemed a successful
optimization and enhancement of the program.

% }}}

\bibliography{mppcw.bib}

\end{document}
